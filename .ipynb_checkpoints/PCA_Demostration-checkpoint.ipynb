{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bcb372",
   "metadata": {},
   "source": [
    "# The MNIST Digits Database\n",
    "\n",
    "We will work with a simple image dataset: the MNIST (**Mixed National Institute of Standards and Technology**) database of handwritten digits, one of the best known datasets in machine learning.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png)\n",
    "\n",
    "https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f68d30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd7bce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from optree->keras) (4.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "447421d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58846430",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from scipy.stats import pearsonr\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Algos'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01034b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "(X_train, y_train), (X_test, y_test) = tk.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "X_validation = X_train[50000:]\n",
    "X_validation = X_validation.reshape((10000,784))\n",
    "\n",
    "y_validation = y_train[50000:]\n",
    "\n",
    "X_train = X_train[:50000]\n",
    "X_train = X_train.reshape((50000,784))\n",
    "\n",
    "X_test = X_test.reshape((10000,784))\n",
    "\n",
    "y_train = y_train[:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shape of datasets\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_validation: \", X_validation.shape)\n",
    "print(\"Shape of y_validation: \", y_validation.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)\n",
    "Shape of X_train:  (50000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation), len(X_train)+len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca768832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the training matrix\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf03113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the labels\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(example):\n",
    "  label = y_train.loc[example]\n",
    "  image = X_train.loc[example,:].values.reshape([28,28])\n",
    "  plt.title('Example: %d Label: %d' % (example, label))\n",
    "  plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "  plt.show()\n",
    "\n",
    "view_digit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721d7a2",
   "metadata": {},
   "source": [
    "## **PCA in Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set the hyperparameters'''\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 784\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Apply PCA'''\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate PCA'''\n",
    "# Percentage of Variance Captured by 784 principal components\n",
    "print(\"Variance Explained by all 784 principal components: \", \\\n",
    "sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Variance Captured by X principal components\n",
    "importanceOfPrincipalComponents = \\\n",
    "pd.DataFrame(data=pca.explained_variance_ratio_)\n",
    "importanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n",
    "\n",
    "print('Variance Captured by First 10 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\n",
    "print('Variance Captured by First 20 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\n",
    "print('Variance Captured by First 50 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\n",
    "print('Variance Captured by First 100 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\n",
    "print('Variance Captured by First 200 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\n",
    "print('Variance Captured by First 300 Principal Components: ',\n",
    "importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterPlot(xDF, yDF, algoName):\n",
    "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "    data=tempDF, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations using \"+algoName)\n",
    "scatterPlot(X_train_PCA, y_train, \"PCA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5daf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View two random dimensions\n",
    "X_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\n",
    "X_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\n",
    "X_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Separation of Observations Using Original Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd4a21",
   "metadata": {},
   "source": [
    "## **Incremental PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_components = 784\n",
    "batch_size = None\n",
    "\n",
    "incrementalPCA = IncrementalPCA(n_components=n_components, \\\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\n",
    "X_train_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n",
    "\n",
    "X_validation_incrementalPCA = incrementalPCA.transform(X_validation)\n",
    "X_validation_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186717bd",
   "metadata": {},
   "source": [
    "## **Sparse PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b361da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 100\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:10000,:])\n",
    "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
    "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n",
    "\n",
    "X_validation_sparsePCA = sparsePCA.transform(X_validation)\n",
    "X_validation_sparsePCA = \\\n",
    "    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe17a39",
   "metadata": {},
   "source": [
    "## **Kernel PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 100\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "random_state = 2018\n",
    "n_jobs = 1\n",
    "\n",
    "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
    "                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "kernelPCA.fit(X_train.loc[:10000,:])\n",
    "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
    "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n",
    "\n",
    "X_validation_kernelPCA = kernelPCA.transform(X_validation)\n",
    "X_validation_kernelPCA = \\\n",
    "    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200c456",
   "metadata": {},
   "source": [
    "## **Singular Value Decomposition**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 200\n",
    "algorithm = 'randomized'\n",
    "n_iter = 5\n",
    "random_state = 2018\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n",
    "                   n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n",
    "\n",
    "X_validation_svd = svd.transform(X_validation)\n",
    "X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016f9d5",
   "metadata": {},
   "source": [
    "## **Gaussian Random Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Random Projection\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "eps = 0.5\n",
    "random_state = 2018\n",
    "\n",
    "GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n",
    "                               random_state=random_state)\n",
    "\n",
    "X_train_GRP = GRP.fit_transform(X_train)\n",
    "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n",
    "\n",
    "X_validation_GRP = GRP.transform(X_validation)\n",
    "X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfb9c5",
   "metadata": {},
   "source": [
    "## **Sparse Random Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Random Projection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "density = 'auto'\n",
    "eps = 0.5\n",
    "dense_output = False\n",
    "random_state = 2018\n",
    "\n",
    "SRP = SparseRandomProjection(n_components=n_components, \\\n",
    "        density=density, eps=eps, dense_output=dense_output, \\\n",
    "        random_state=random_state)\n",
    "\n",
    "X_train_SRP = SRP.fit_transform(X_train)\n",
    "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n",
    "\n",
    "X_validation_SRP = SRP.transform(X_validation)\n",
    "X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6049d8",
   "metadata": {},
   "source": [
    "# **Non-Linear Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be6306",
   "metadata": {},
   "source": [
    "## **Isomap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isomap\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "n_neighbors = 5\n",
    "n_components = 10\n",
    "n_jobs = 4\n",
    "\n",
    "isomap = Isomap(n_neighbors=n_neighbors, \\\n",
    "                n_components=n_components, n_jobs=n_jobs)\n",
    "\n",
    "isomap.fit(X_train.loc[0:5000,:])\n",
    "X_train_isomap = isomap.transform(X_train)\n",
    "X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n",
    "\n",
    "X_validation_isomap = isomap.transform(X_validation)\n",
    "X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n",
    "                                   index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_isomap, y_train, \"Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9720b",
   "metadata": {},
   "source": [
    "## **Multidimensional Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8789c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidimensional Scaling\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "n_components = 2\n",
    "n_init = 12\n",
    "max_iter = 1200\n",
    "metric = True\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\n",
    "          metric=metric, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\n",
    "X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\n",
    "\n",
    "scatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a9805",
   "metadata": {},
   "source": [
    "## **Locally Linear Embedding (LLE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Linear Embedding (LLE)\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "method = 'modified'\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\n",
    "        n_components=n_components, method=method, \\\n",
    "        random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "lle.fit(X_train.loc[0:5000,:])\n",
    "X_train_lle = lle.transform(X_train)\n",
    "X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\n",
    "\n",
    "X_validation_lle = lle.transform(X_validation)\n",
    "X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb78729",
   "metadata": {},
   "source": [
    "## **t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500daba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "n_components = 2\n",
    "learning_rate = 300\n",
    "perplexity = 30\n",
    "early_exaggeration = 12\n",
    "init = 'random'\n",
    "random_state = 2018\n",
    "\n",
    "tSNE = TSNE(n_components=n_components, learning_rate=learning_rate, \\\n",
    "            perplexity=perplexity, early_exaggeration=early_exaggeration, \\\n",
    "            init=init, random_state=random_state)\n",
    "\n",
    "X_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:5000,:9])\n",
    "X_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:5001])\n",
    "\n",
    "scatterPlot(X_train_tSNE, y_train, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef0e80",
   "metadata": {},
   "source": [
    "# **Other Dimensionality Reduction Algos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d84478",
   "metadata": {},
   "source": [
    "## **Dictionary Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = 50\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 25\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "                        n_components=n_components, alpha=alpha, \\\n",
    "                        batch_size=batch_size, n_iter=n_iter, \\\n",
    "                        random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_train.loc[:,:10000])\n",
    "X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)\n",
    "X_train_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_train_miniBatchDictLearning, index=train_index)\n",
    "\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    miniBatchDictLearning.transform(X_validation)\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    pd.DataFrame(data=X_validation_miniBatchDictLearning, \\\n",
    "    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_miniBatchDictLearning, y_train, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e6ad1",
   "metadata": {},
   "source": [
    "## **Independent Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a58563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "n_components = 25\n",
    "algorithm = 'parallel'\n",
    "whiten = True\n",
    "max_iter = 100\n",
    "random_state = 2018\n",
    "\n",
    "fastICA = FastICA(n_components=n_components, algorithm=algorithm, \\\n",
    "                  whiten=whiten, max_iter=max_iter, random_state=random_state)\n",
    "\n",
    "X_train_fastICA = fastICA.fit_transform(X_train)\n",
    "X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)\n",
    "\n",
    "X_validation_fastICA = fastICA.transform(X_validation)\n",
    "X_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \\\n",
    "                                    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef14a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e78334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
